{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77eef044-11dd-4a04-b0f8-2fc9aa1523f5",
   "metadata": {},
   "source": [
    "# Real-world case: Credit scoring model\n",
    "### Author: MartÃ­n Anaya\n",
    "\n",
    "In this second experiment, we transition from synthetic data to a real-world scenario by using the German Credit Dataset. This dataset contains information on 1000 individuals, including sensitive attributes such as gender, alongside a label indicating their creditworthiness. More details about this dataset and its full set of attributes can be found in https://archive.ics.uci.edu/dataset/144/statlog+german+credit+data.\n",
    "\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ef7324-27bc-46bf-96a8-7f08b99cafbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilities\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Data Loading\n",
    "from ucimlrepo import fetch_ucirepo \n",
    "\n",
    "# Data preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Logistic regression model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Fairlearn\n",
    "from fairlearn.reductions import ExponentiatedGradient, DemographicParity\n",
    "\n",
    "# Evaluation metrics\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score\n",
    "\n",
    "# In order to improve the notebook's readability, we ignore the max_iter warnings.\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fbbe38-06d3-42e9-b0ed-3a52b61bb40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=42 # Setting a seed for reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce42d289-d68d-4525-906b-0a6bd373d055",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25c1b05-cc91-461c-ae53-6d72c1b77c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch dataset \n",
    "statlog_german_credit_data = fetch_ucirepo(id=144) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X = statlog_german_credit_data.data.features \n",
    "y = statlog_german_credit_data.data.targets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da27983-4d44-4761-8165-405bb043c495",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([X,y], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b869306f-3e26-45a8-80e8-e66dbc7ea726",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ec54cf-3fa8-404c-951e-7131e351ba40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We rename the columns\n",
    "df.columns = [\n",
    "    \"checking_status\",  # A1\n",
    "    \"duration\",         # A2\n",
    "    \"credit_history\",   # A3\n",
    "    \"purpose\",          # A4\n",
    "    \"credit_amount\",    # A5\n",
    "    \"savings_status\",   # A6\n",
    "    \"employment\",       # A7\n",
    "    \"installment_commitment\",  # A8\n",
    "    \"personal_status\",        # A9\n",
    "    \"other_parties\",          # A10\n",
    "    \"residence_since\",        # A11\n",
    "    \"property_magnitude\",     # A12\n",
    "    \"age\",                    # A13\n",
    "    \"other_payment_plans\",    # A14\n",
    "    \"housing\",                # A15\n",
    "    \"existing_credits\",       # A16\n",
    "    \"job\",                    # A17\n",
    "    \"num_dependents\",         # A18\n",
    "    \"own_telephone\",          # A19\n",
    "    \"foreign_worker\",         # A20\n",
    "    \"class\"                   # target\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956b83f9-2e45-4015-8910-5a0bada9746d",
   "metadata": {},
   "source": [
    "The column of relevant interest to us is _personal_status_. This contains both the gender of the individual and their marital status. We will decode this attribute and create two separate features for each dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa8048f-e9ae-476b-96d1-a1fda30922c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_personal_status = {\n",
    "    \"A91\": {\"sex\": \"male\",   \"marital_status\": \"divorced/separated\"},\n",
    "    \"A92\": {\"sex\": \"female\", \"marital_status\": \"divorced/separated/married\"},\n",
    "    \"A93\": {\"sex\": \"male\",   \"marital_status\": \"single\"},\n",
    "    \"A94\": {\"sex\": \"male\",   \"marital_status\": \"married/widowed\"},\n",
    "    \"A95\": {\"sex\": \"female\", \"marital_status\": \"single\"}\n",
    "}\n",
    "\n",
    "# Tranform the labels of the dataset from 1 and 2 to 0 and 1.\n",
    "df[\"class\"] = df[\"class\"].map({1: 1, 2: 0}) #1 = Good, 2 = Bad => 1 = Positive, 0 = Negative\n",
    "\n",
    "# Map personal_status\n",
    "df[\"sex\"] = df[\"personal_status\"].map(lambda x: mapping_personal_status[x][\"sex\"])\n",
    "df[\"marital_status\"] = df[\"personal_status\"].map(lambda x: mapping_personal_status[x][\"marital_status\"])\n",
    "\n",
    "# Remove the column\n",
    "df = df.drop(\"personal_status\" , axis = 1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a30bfb2-99ca-4122-9fa0-d5e27903c6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Female count: \", len(df[df[\"sex\"]==\"female\"]))\n",
    "print(\"Male count: \", len(df[df[\"sex\"]==\"male\"]))\n",
    "\n",
    "print(\"\\n\\nOutcome mean by sex (1=credit, 0=no_credit):\")\n",
    "print(df.groupby(\"sex\")[\"class\"].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e8fdab-ecd2-4187-8741-eade0047fadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.drop(columns=[\"class\"]), df[\"class\"], test_size=0.5, random_state=42, stratify=df[\"class\"])\n",
    "\n",
    "print(\"Female count in X_train: \", len(X_train[X_train[\"sex\"]==\"female\"]))\n",
    "print(\"Male count in X_train: \", len(X_train[X_train[\"sex\"]==\"male\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f319646-bc80-47a3-b0cb-166e059fc381",
   "metadata": {},
   "source": [
    "## SPRT Framework\n",
    "\n",
    "We adapted the code from the previous notebook to the credit scoring dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4bb3726-bda3-47d9-921d-52f6b0ce136e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sprt_statistical_parity(group1, group2, alpha=0.05, beta=0.2, delta=0.1):\n",
    "    \"\"\"\n",
    "    SPRT for Statistical Parity fairness criterion.\n",
    "    \"\"\"\n",
    "    n1 = len(group1)\n",
    "    n2 = len(group2)\n",
    "\n",
    "    s1 = group1['prediction'].sum() \n",
    "    s2 = group2['prediction'].sum() \n",
    "\n",
    "    p1_hat = s1 / n1\n",
    "    p2_hat = s2 / n2\n",
    "\n",
    "    p0 = (p1_hat + p2_hat) / 2 \n",
    "    p1 = p0 + delta/2\n",
    "    p2 = p0 - delta/2\n",
    "\n",
    "    # Likelihood ratio\n",
    "    L0 = (p0**s1 * (1 - p0)**(n1 - s1)) * (p0**s2 * (1 - p0)**(n2 - s2))\n",
    "    L1 = (p1**s1 * (1 - p1)**(n1 - s1)) * (p2**s2 * (1 - p2)**(n2 - s2))\n",
    "    lr = L1 / L0 if L0 > 0 else np.inf\n",
    "\n",
    "    # Thresholds\n",
    "    A = (1 - beta) / alpha\n",
    "    B = beta / (1 - alpha)\n",
    "\n",
    "    # Decision\n",
    "    if lr >= A:\n",
    "        return \"Reject H0 (difference detected)\", lr\n",
    "    elif lr <= B:\n",
    "        return \"Accept H0 (no difference)\", lr\n",
    "    else:\n",
    "        return \"Continue sampling\", lr\n",
    "\n",
    "\n",
    "def sprt_equal_opportunity(group1, group2, alpha=0.05, beta=0.2, delta=0.1):\n",
    "    \"\"\"\n",
    "    SPRT for Equal Opportunity fairness criterion.\n",
    "    Compares true positive rates (TPR) between two groups.\n",
    "    \"\"\"\n",
    "\n",
    "    # Filter only the truly positive individuals (Y=1)\n",
    "    g1_pos = group1[group1['prediction'] == 1]\n",
    "    g2_pos = group2[group2['prediction'] == 1]\n",
    "    \n",
    "    n1 = len(g1_pos)\n",
    "    n2 = len(g2_pos)\n",
    "\n",
    "    # We need one sample from each group\n",
    "    if n1 == 0 or n2 == 0:\n",
    "        return \"Continue sampling\" , np.nan\n",
    "\n",
    "\n",
    "    s1 = group1['prediction'].sum() \n",
    "    s2 = group2['prediction'].sum() \n",
    "\n",
    "    # Observed TPRs\n",
    "    p1_hat = s1 / n1\n",
    "    p2_hat = s2 / n2\n",
    "\n",
    "    p0 = (p1_hat + p2_hat) / 2 \n",
    "    p1 = p0 + delta/2\n",
    "    p2 = p0 - delta/2\n",
    "\n",
    "    # Likelihood ratio\n",
    "    L0 = (p0**s1 * (1 - p0)**(n1 - s1)) * (p0**s2 * (1 - p0)**(n2 - s2))\n",
    "    L1 = (p1**s1 * (1 - p1)**(n1 - s1)) * (p2**s2 * (1 - p2)**(n2 - s2))\n",
    "    lr = L1 / L0 if L0 > 0 else np.inf\n",
    "\n",
    "    # Thresholds\n",
    "    A = (1 - beta) / alpha\n",
    "    B = beta / (1 - alpha)\n",
    "\n",
    "    # Decision\n",
    "    if lr >= A:\n",
    "        return \"Reject H0 (difference detected)\", lr\n",
    "    elif lr <= B:\n",
    "        return \"Accept H0 (no difference)\", lr\n",
    "    else:\n",
    "        return \"Continue sampling\", lr\n",
    "\n",
    "\n",
    "# SPRT main loop\n",
    "def run_test(df, batch_size=10, max_steps=1000, alpha = 0.05, beta = 0.2, delta=0.1, criterion=\"SP\"):\n",
    "    pool = df.copy()\n",
    "    pool = pool.sample(frac=1).reset_index(drop=True) # reordena para usar iloc\n",
    "\n",
    "    results = []\n",
    "\n",
    "    g1_index = pool[pool['sex'] == 'male'].index[0]\n",
    "    g2_index = pool[pool['sex'] == 'female'].index[0]\n",
    "    \n",
    "    # Evidence set\n",
    "    accumulated = pool.loc[[g1_index, g2_index]]\n",
    "    pool = pool.drop([g1_index, g2_index])\n",
    "\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        if len(pool) < batch_size:\n",
    "            #print(\"Not enough data\")\n",
    "            break\n",
    "\n",
    "        batch = pool.iloc[:batch_size]\n",
    "        pool = pool.iloc[batch_size:]\n",
    "        \n",
    "        accumulated = pd.concat([accumulated, batch], ignore_index=True)\n",
    "\n",
    "        g1 = accumulated[accumulated['sex'] == 'male']\n",
    "        g2 = accumulated[accumulated['sex'] == 'female']\n",
    "        \n",
    "        # Choose test depending on criterion\n",
    "        if criterion == \"SP\":\n",
    "            decision, lr = sprt_statistical_parity(g1, g2, alpha=alpha, beta=beta, delta=delta)\n",
    "        elif criterion == \"EO\":\n",
    "            decision, lr = sprt_equal_opportunity(g1, g2, alpha=alpha, beta=beta, delta=delta)\n",
    "        else:\n",
    "            raise ValueError(\"criterion must be 'SP' or 'EO'\")\n",
    "\n",
    "        results.append((step+1, decision, lr))\n",
    "        \n",
    "        if decision != \"Continue sampling\":\n",
    "            break\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95dbc2ba-46ee-4858-8e6c-b7fa1e480f20",
   "metadata": {},
   "source": [
    "## Experiments\n",
    "\n",
    "The hypotheses for our test are defined as:\n",
    "\n",
    "$$H_0 : \\pi_1 = \\pi_2 \\qquad \\text{(the system is fair)}$$\n",
    "\n",
    "$$H_1 : |\\pi_1 - \\pi_2| \\geq 0.1  \\qquad \\text{(the system favours one group over the other)}$$\n",
    "\n",
    "where:\n",
    "$$\\pi_1 = P(\\text{credit approved | A = Male})$$ \n",
    "$$\\pi_2 = P(\\text{credit approved | A = Female})$$\n",
    "\n",
    "\n",
    "### Baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f973393-fb39-4049-ae83-bdc33c36b339",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = X_train.select_dtypes(include=\"object\").columns\n",
    "numeric_cols = X_train.select_dtypes(exclude=\"object\").columns\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), categorical_cols),\n",
    "        (\"num\", \"passthrough\", numeric_cols)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Base model\n",
    "clf = Pipeline(steps=[\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"classifier\", LogisticRegression(max_iter=1000, random_state=seed))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41d4f22-596a-4202-928e-d5f9a295402e",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X_train, y_train)\n",
    "print(\"Train acc:\", clf.score(X_train, y_train))\n",
    "print(\"Test acc:\", clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3943f93-341e-41db-bf65-b71db0316f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "output_classifier = X_test.copy()\n",
    "output_classifier[\"true\"] = y_test\n",
    "output_classifier[\"prediction\"] = y_pred\n",
    "output_classifier[\"sex\"] = df.loc[X_test.index, \"sex\"]\n",
    "\n",
    "output_classifier = output_classifier.loc[:, [\"true\",\"prediction\",\"sex\"]]\n",
    "\n",
    "output_classifier.groupby(\"sex\")[\"prediction\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3263d46-bda4-40e5-908d-d51ed432f1b6",
   "metadata": {},
   "source": [
    "### Classifier without sensitive attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658a38e4-62ee-4988-ac94-9324b8703d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_no_sentitive = X_train.drop(columns=[\"sex\"])\n",
    "X_test_no_sensitive = X_test.drop(columns=[\"sex\"])\n",
    "\n",
    "categorical_cols_no_sensitive = X_train_no_sentitive.select_dtypes(include=\"object\").columns\n",
    "numeric_cols_no_sensitive = X_train_no_sentitive.select_dtypes(exclude=\"object\").columns\n",
    "\n",
    "\n",
    "preprocessor_no_sensitive = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), categorical_cols_no_sensitive),\n",
    "        (\"num\", \"passthrough\", numeric_cols_no_sensitive)\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "X_train_no_sensitive_enc = preprocessor_no_sensitive.fit_transform(X_train_no_sentitive)\n",
    "X_test_no_sensitive_enc = preprocessor_no_sensitive.transform(X_test_no_sensitive)\n",
    "\n",
    "\n",
    "no_sensitive_clf = Pipeline(steps=[\n",
    "    (\"preprocessor\", preprocessor_no_sensitive),\n",
    "    (\"classifier\", LogisticRegression(max_iter=10000, random_state=seed))\n",
    "])\n",
    "\n",
    "no_sensitive_clf.fit(X_train_no_sentitive, y_train)\n",
    "print(\"Train acc:\", no_sensitive_clf.score(X_train_no_sentitive, y_train))\n",
    "print(\"Test acc:\", no_sensitive_clf.score(X_test_no_sensitive, y_test))\n",
    "\n",
    "y_pred_no_sensitive = no_sensitive_clf.predict(X_test_no_sensitive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fcdc42f-5cad-482c-9fce-0f94c9dd6897",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_classifier_no_sensitive = X_test.copy()\n",
    "output_classifier_no_sensitive[\"true\"] = y_test\n",
    "output_classifier_no_sensitive[\"prediction\"] = y_pred_no_sensitive\n",
    "output_classifier_no_sensitive[\"sex\"] = df.loc[X_test.index, \"sex\"]\n",
    "output_classifier_no_sensitive = output_classifier_no_sensitive.loc[:, [\"true\",\"prediction\",\"sex\"]]\n",
    "\n",
    "output_classifier_no_sensitive.groupby(\"sex\")[\"prediction\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab676d1e-7801-4b69-b453-ed4456f23ae1",
   "metadata": {},
   "source": [
    "### Fairness-aware classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf4fc56-616b-44bf-b3cd-3f586b873f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer([\n",
    "    (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), categorical_cols),\n",
    "    (\"num\", \"passthrough\", numeric_cols)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f88b47-93b3-4c3f-8047-9ad48fa6d1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_enc = preprocessor.fit_transform(X_train)\n",
    "X_test_enc = preprocessor.transform(X_test)\n",
    "\n",
    "base_clf = LogisticRegression(max_iter=5000, random_state=seed)\n",
    "\n",
    "# Fair classifier\n",
    "fair_clf = ExponentiatedGradient(\n",
    "    estimator=base_clf,\n",
    "    constraints=DemographicParity()\n",
    ")\n",
    "\n",
    "# Train\n",
    "fair_clf.fit(\n",
    "    X_train_enc,\n",
    "    y_train,\n",
    "    sensitive_features=X_train[\"sex\"]\n",
    ")\n",
    "\n",
    "# Predictions\n",
    "y_pred_fair = fair_clf.predict(X_test_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60635104-a617-4e7a-a279-16814a3ca911",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_classifier_fair = X_test.copy()\n",
    "output_classifier_fair[\"true\"] = y_test\n",
    "output_classifier_fair[\"prediction\"] = y_pred_fair\n",
    "output_classifier_fair[\"sex\"] = df.loc[X_test.index, \"sex\"]\n",
    "output_classifier_fair = output_classifier_fair.loc[:, [\"true\",\"prediction\",\"sex\"]]\n",
    "\n",
    "\n",
    "output_classifier_fair.groupby(\"sex\")[\"prediction\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9deaf7-e3ca-43da-9d55-665ce3099c8b",
   "metadata": {},
   "source": [
    "### Fairness-accuracy tradeoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a8fefa-c0bb-45df-b6c8-26b179e55fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(y_true, y_pred):\n",
    "    print(f\"Accuracy: {accuracy_score(y_true, y_pred):.2f}, Precision: {precision_score(y_true, y_pred):.2f}, Recall: {recall_score(y_true, y_pred):.2f}, F1-Score: {f1_score(y_true, y_pred):.2f}\")\n",
    "\n",
    "\n",
    "print(\"\\nBaseline model:\")\n",
    "metrics(y_test, y_pred)\n",
    "\n",
    "print(\"\\nNo sensitive attributes model:\")\n",
    "metrics(y_test, y_pred_no_sensitive)\n",
    "\n",
    "print(\"\\nFair model:\")\n",
    "metrics(y_test, y_pred_fair)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0c182a-b6c1-4a21-839c-e078efe131c3",
   "metadata": {},
   "source": [
    "### Audit simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc018e10-9d4c-45fe-93db-e3cfc80e5453",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5\n",
    "max_steps = 2000\n",
    "num_simulations = 10000\n",
    "\n",
    "alpha = 0.05\n",
    "beta = 0.2\n",
    "delta = 0.1\n",
    "\n",
    "criterion = 'SP'\n",
    "\n",
    "test_df = output_classifier_fair\n",
    "\n",
    "\n",
    "\n",
    "def simulation_loop(num_simulations, test_df, batch_size, max_steps, alpha, beta, delta, criterion):\n",
    "    resultados_simulaciones = pd.DataFrame()\n",
    "    count_data_limit_reached = 0\n",
    "    for i in range(num_simulations): \n",
    "        res = run_test(df = test_df, batch_size = batch_size, max_steps = max_steps, alpha = alpha, beta = beta, delta = delta, criterion = criterion)[-1][0:2]\n",
    "        res = pd.DataFrame(res).transpose()\n",
    "        \n",
    "        col = res.columns[1]\n",
    "\n",
    "        res.loc[res[col] == 'Reject H0 (difference detected)', col] = 1\n",
    "        res.loc[res[col] == 'Accept H0 (no difference)', col] = 0\n",
    "        \n",
    "        if(res[1] == 'Continue sampling').any():\n",
    "           count_data_limit_reached = count_data_limit_reached+1\n",
    "            \n",
    "        res.loc[res[col] == 'Continue sampling', col] = 0\n",
    "        \n",
    "        res.iloc[:,0] = res.iloc[:,0] * batch_size + 2\n",
    "            \n",
    "        resultados_simulaciones = pd.concat([resultados_simulaciones, res], ignore_index=True)\n",
    "    \n",
    "    resultados_simulaciones.columns = [\"Num_samples\", \"Difference Detected\"]\n",
    "    \n",
    "    print(\"Num simulaciones:\" , num_simulations, \"\\nMean sample size: \", resultados_simulaciones['Num_samples'].mean(), \"\\nBias detected: \", resultados_simulaciones['Difference Detected'].sum(),\"\\nNot enough data: \", count_data_limit_reached)\n",
    "    return resultados_simulaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d944e59-db1e-4ea5-a2d9-ac985a39edbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.05\n",
    "beta = 0.2\n",
    "batch_size = 15\n",
    "\n",
    "print(\"Base classifier:\")\n",
    "base_res = simulation_loop(num_simulations, output_classifier, batch_size, max_steps, alpha, beta, delta, criterion)\n",
    "print(\"\\nClassifier without sensitive attributes:\")\n",
    "no_sensitive_res = simulation_loop(num_simulations, output_classifier_no_sensitive, batch_size, max_steps, alpha, beta, delta, criterion)\n",
    "print(\"\\nFair classifier:\")\n",
    "fair_res = simulation_loop(num_simulations, output_classifier_fair, batch_size, max_steps, alpha, beta, delta, criterion)\n",
    "\n",
    "alpha = 0.05\n",
    "beta = 0.1\n",
    "print(\"Base classifier:\")\n",
    "base_res = simulation_loop(num_simulations, output_classifier, batch_size, max_steps, alpha, beta, delta, criterion)\n",
    "print(\"\\nClassifier without sensitive attributes:\")\n",
    "no_sensitive_res = simulation_loop(num_simulations, output_classifier_no_sensitive, batch_size, max_steps, alpha, beta, delta, criterion)\n",
    "print(\"\\nFair classifier:\")\n",
    "fair_res = simulation_loop(num_simulations, output_classifier_fair, batch_size, max_steps, alpha, beta, delta, criterion)\n",
    "\n",
    "alpha = 0.1\n",
    "beta = 0.2\n",
    "print(\"Base classifier:\")\n",
    "base_res = simulation_loop(num_simulations, output_classifier, batch_size, max_steps, alpha, beta, delta, criterion)\n",
    "print(\"\\nClassifier without sensitive attributes:\")\n",
    "no_sensitive_res = simulation_loop(num_simulations, output_classifier_no_sensitive, batch_size, max_steps, alpha, beta, delta, criterion)\n",
    "print(\"\\nFair classifier:\")\n",
    "fair_res = simulation_loop(num_simulations, output_classifier_fair, batch_size, max_steps, alpha, beta, delta, criterion)\n",
    "\n",
    "alpha = 0.1\n",
    "beta = 0.1\n",
    "print(\"Base classifier:\")\n",
    "base_res = simulation_loop(num_simulations, output_classifier, batch_size, max_steps, alpha, beta, delta, criterion)\n",
    "print(\"\\nClassifier without sensitive attributes:\")\n",
    "no_sensitive_res = simulation_loop(num_simulations, output_classifier_no_sensitive, batch_size, max_steps, alpha, beta, delta, criterion)\n",
    "print(\"\\nFair classifier:\")\n",
    "fair_res = simulation_loop(num_simulations, output_classifier_fair, batch_size, max_steps, alpha, beta, delta, criterion)\n",
    "\n",
    "\n",
    "\n",
    "alpha = 0.05\n",
    "beta = 0.2\n",
    "batch_size = 5\n",
    "\n",
    "print(\"Base classifier:\")\n",
    "base_res = simulation_loop(num_simulations, output_classifier, batch_size, max_steps, alpha, beta, delta, criterion)\n",
    "print(\"\\nClassifier without sensitive attributes:\")\n",
    "no_sensitive_res = simulation_loop(num_simulations, output_classifier_no_sensitive, batch_size, max_steps, alpha, beta, delta, criterion)\n",
    "print(\"\\nFair classifier:\")\n",
    "fair_res = simulation_loop(num_simulations, output_classifier_fair, batch_size, max_steps, alpha, beta, delta, criterion)\n",
    "\n",
    "alpha = 0.05\n",
    "beta = 0.1\n",
    "print(\"Base classifier:\")\n",
    "base_res = simulation_loop(num_simulations, output_classifier, batch_size, max_steps, alpha, beta, delta, criterion)\n",
    "print(\"\\nClassifier without sensitive attributes:\")\n",
    "no_sensitive_res = simulation_loop(num_simulations, output_classifier_no_sensitive, batch_size, max_steps, alpha, beta, delta, criterion)\n",
    "print(\"\\nFair classifier:\")\n",
    "fair_res = simulation_loop(num_simulations, output_classifier_fair, batch_size, max_steps, alpha, beta, delta, criterion)\n",
    "\n",
    "alpha = 0.1\n",
    "beta = 0.2\n",
    "print(\"Base classifier:\")\n",
    "base_res = simulation_loop(num_simulations, output_classifier, batch_size, max_steps, alpha, beta, delta, criterion)\n",
    "print(\"\\nClassifier without sensitive attributes:\")\n",
    "no_sensitive_res = simulation_loop(num_simulations, output_classifier_no_sensitive, batch_size, max_steps, alpha, beta, delta, criterion)\n",
    "print(\"\\nFair classifier:\")\n",
    "fair_res = simulation_loop(num_simulations, output_classifier_fair, batch_size, max_steps, alpha, beta, delta, criterion)\n",
    "\n",
    "alpha = 0.1\n",
    "beta = 0.1\n",
    "print(\"Base classifier:\")\n",
    "base_res = simulation_loop(num_simulations, output_classifier, batch_size, max_steps, alpha, beta, delta, criterion)\n",
    "print(\"\\nClassifier without sensitive attributes:\")\n",
    "no_sensitive_res = simulation_loop(num_simulations, output_classifier_no_sensitive, batch_size, max_steps, alpha, beta, delta, criterion)\n",
    "print(\"\\nFair classifier:\")\n",
    "fair_res = simulation_loop(num_simulations, output_classifier_fair, batch_size, max_steps, alpha, beta, delta, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d41dbb7-6c35-4850-8b97-36d47ceb8117",
   "metadata": {},
   "source": [
    "### Audit another fairness criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45790fbf-a394-4823-86c4-b10c5f84848b",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.05\n",
    "beta = 0.2\n",
    "batch_size = 5\n",
    "delta = 0.5\n",
    "num_simulations = 1000\n",
    "criterion = 'EO'\n",
    "\n",
    "print(\"\\nFair classifier:\")\n",
    "fair_eop = simulation_loop(num_simulations, output_classifier_fair, batch_size, max_steps, alpha, beta, delta, criterion)\n",
    "\n",
    "alpha = 0.1\n",
    "beta = 0.2\n",
    "print(\"\\nFair classifier:\")\n",
    "fair_eop = simulation_loop(num_simulations, output_classifier_fair, batch_size, max_steps, alpha, beta, delta, criterion)\n",
    "\n",
    "alpha = 0.05\n",
    "beta = 0.1\n",
    "print(\"\\nFair classifier:\")\n",
    "fair_eop = simulation_loop(num_simulations, output_classifier_fair, batch_size, max_steps, alpha, beta, delta, criterion)\n",
    "\n",
    "alpha = 0.1\n",
    "beta = 0.1\n",
    "print(\"\\nFair classifier:\")\n",
    "fair_eop = simulation_loop(num_simulations, output_classifier_fair, batch_size, max_steps, alpha, beta, delta, criterion)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecf5682c-e885-47b1-9d97-806b34d6686d",
   "metadata": {},
   "source": [
    "## Sequential Hypothesis Testing for AI Auditing\n",
    "### Author: Mart√≠n Anaya\n",
    "\n",
    "In this notebook, we design controlled experiments using synthetic data and simulated black-box models. The purpose of these experiments is to evaluate the Sequential Probability Ratio Test (SPRT) under reproducible and well-defined conditions as an auditing tool.\n",
    "\n",
    "### Library Imports\n",
    "We begin by loading the libraries that we will use in this experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305d4577-5d6b-473a-8a04-c26c20d37f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilities\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Statistical libraries\n",
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64abc776-4f57-4b03-993c-9f903fa03f62",
   "metadata": {},
   "source": [
    "### Simulation of black-box outputs\n",
    "\n",
    "In this framework, the black-box model is represented as a probabilistic mapping from pre-defined biased probabilities to system outcomes. More precisely, we define a probabilistic setup in which individuals are assigned a likelihood of being selected or rejected depending on an attribute noted _suitability_. This attribute allows us to find the true positive and true negative outcomes of the model. By controlling these probabilities, we are able to introduce systematic bias into the model while maintaining transparency over the process of data generation.\n",
    "\n",
    "\n",
    "To operationalise the model, we generate two sets of probabilities:\n",
    "\n",
    "- Suitability rates: Defines the probability of an individual belonging to the \"suitable\" group. We assume both groups to have the same likelihood of being truly suitable.\n",
    "    \n",
    "    $$\\mathbb{P}(\\text{suitable } | \\text{ Group = 1}) = 0.8 \\qquad \\mathbb{P}(\\text{suitable } | \\text{ Group = 2}) = 0.8$$\n",
    "\n",
    "- Bias rates: Defines the probability that the model assigns an individual to the positive class depending on both their group and their true suitability. \n",
    "    $$\\mathbb{P}(\\text{positive } | \\text{ Group = 1, suitable}) = 0.8 \\qquad \\mathbb{P}(\\text{positive } | \\text{ Group = 1, unsuitable}) = 0.25$$\n",
    "    $$\\mathbb{P}(\\text{positive } | \\text{ Group = 2, suitable}) = 0.5 \\qquad \\mathbb{P}(\\text{positive } | \\text{ Group = 2, unsuitable}) = 0.1$$\n",
    "\n",
    "\n",
    "This setup ensures that, despite that both groups contain the same proportion of qualified individuals, the model systematically favours Group 1 over Group 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f353ac22-b97d-4c56-b949-5e8e9a8cd9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to simulate the black-box model\n",
    "def black_box_outputs(num_group1, num_group2, suitability_rates, bias_rates, seed = 42):\n",
    "    \"\"\"\n",
    "        Parameters: \n",
    "        num_group1: number of individuals from group 1\n",
    "        num_group2: number of individuals from group 2\n",
    "\n",
    "        suitability_rates: Probabilities of individuals from each group to belong to the suitable group\n",
    "        bias_rates: Probabilities of the model assigning an individual to the positive class depending on their group and suitability\n",
    "\n",
    "        returns:\n",
    "        dataframe with the outputs of the system\n",
    "        \n",
    "    \"\"\"\n",
    "    random_generator = np.random.default_rng(seed)\n",
    "\n",
    "    # Probabilistic suitability rate\n",
    "    num_group1_suitable = sum(random_generator.random(num_group1) < suitability_rates['group1_prob_suitable'])\n",
    "    num_group1_unsuitable = num_group1 - num_group1_suitable\n",
    "\n",
    "    num_group2_suitable = sum(random_generator.random(num_group2) < suitability_rates['group2_prob_suitable'])\n",
    "    num_group2_unsuitable = num_group2 - num_group2_suitable\n",
    "\n",
    "    # Probabilistic bias rate\n",
    "    positives_group1_suitable = sum(random_generator.random(num_group1_suitable) < bias_rates['group1_suitable'])\n",
    "    positives_group1_unsuitable = sum(random_generator.random(num_group1_unsuitable) < bias_rates['group1_unsuitable'])\n",
    "    positives_group2_suitable = sum(random_generator.random(num_group2_suitable) < bias_rates['group2_suitable'])\n",
    "    positives_group2_unsuitable = sum(random_generator.random(num_group2_unsuitable) < bias_rates['group2_unsuitable'])\n",
    "\n",
    "    negatives_group1_suitable = num_group1_suitable - positives_group1_suitable\n",
    "    negatives_group1_unsuitable = num_group1_unsuitable - positives_group1_unsuitable\n",
    "    negatives_group2_suitable = num_group2_suitable - positives_group2_suitable\n",
    "    negatives_group2_unsuitable = num_group2_unsuitable - positives_group2_unsuitable\n",
    "\n",
    "    # Outputs\n",
    "    outputs = [\n",
    "        ['G1_suit', positives_group1_suitable, negatives_group1_suitable],\n",
    "        ['G1_unsuit', positives_group1_unsuitable, negatives_group1_unsuitable],\n",
    "        ['G2_suit', positives_group2_suitable, negatives_group2_suitable],\n",
    "        ['G2_unsuit', positives_group2_unsuitable, negatives_group2_unsuitable]\n",
    "    ]\n",
    "\n",
    "    df = pd.DataFrame(outputs, columns=['group', 'Positive class', 'Negative class'])\n",
    "    return df\n",
    "\n",
    "# Obtain a dataframe with the group, prediction and true label of each individual. Those will be the samples that we will use in the test\n",
    "def get_individual_df(df):\n",
    "    data = []\n",
    "    # Expand each row in individuals\n",
    "    for _, row in df.iterrows():\n",
    "        group_name = row['group']\n",
    "        group = group_name[:2]                      # 'G1' or 'G2'\n",
    "        suitability = 1 if '_suit' in group_name else 0\n",
    "        \n",
    "        # Add positive individuals\n",
    "        data.extend([{\n",
    "            'group': group,\n",
    "            'suitability': suitability,\n",
    "            'prediction': 1\n",
    "        }] * row['Positive class'])\n",
    "    \n",
    "        # Add negative individuals\n",
    "        data.extend([{\n",
    "            'group': group,\n",
    "            'suitability': suitability,\n",
    "            'prediction': 0\n",
    "            }] * row['Negative class'])\n",
    "\n",
    "    return pd.DataFrame(data)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541582ae-3825-45dd-bcf6-15310ecbb25a",
   "metadata": {},
   "source": [
    "#### Bias rate\n",
    "In the following cell, we define the probabilities of our model to generate each of the outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed048090-d076-47eb-b5fd-3c340a8bc68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "simulation_suitability_rates = {'group1_prob_suitable' : 0.8, \n",
    "                      'group2_prob_suitable' : 0.8}\n",
    "\n",
    "simulation_bias_rates = {'group1_suitable' : 0.8,\n",
    "                            'group1_unsuitable' : 0.25,\n",
    "                            'group2_suitable' : 0.5,\n",
    "                            'group2_unsuitable' : 0.1}\n",
    "\n",
    "alpha = 0.05\n",
    "beta = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f005bf-c88b-488e-95be-0d94f1186164",
   "metadata": {},
   "source": [
    "Now we can generate the outputs of the system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46476a24-9cc3-4a6e-a21a-2840a2c96c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_outputs = black_box_outputs(1000,1000,simulation_suitability_rates,simulation_bias_rates, seed = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4794e59c-abf2-4fc5-876c-f3bdc71183fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of outcomes\n",
    "print(df_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b53c0d-9256-405a-acd2-fb6c8fdcf318",
   "metadata": {},
   "source": [
    "#### Output Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4cf970-4394-4309-8aae-412aaa8911af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pie_charts_outputs_global(df):\n",
    "    df_grouped = df.groupby(df['group'].str.slice(0, 2))[['Positive class', 'Negative class']].sum().reset_index()\n",
    "\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    fig.suptitle('Model predictions by group (G1 / G2)', fontsize=16)\n",
    "    \n",
    "    for i, ax in enumerate(axs.flat):\n",
    "        group = df_grouped.loc[i, 'group']\n",
    "        hired = df_grouped.loc[i, 'Positive class']\n",
    "        not_hired = df_grouped.loc[i, 'Negative class']\n",
    "        ax.pie([hired, not_hired],\n",
    "               labels=['Positive class', 'Negative class'],\n",
    "               autopct='%1.1f%%',\n",
    "               colors=sns.color_palette(\"pastel\"),\n",
    "               startangle=90)\n",
    "        ax.set_title(group)\n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.9])\n",
    "    plt.show()\n",
    "\n",
    "def pie_charts_outputs_subgroups(df):\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(10, 8))\n",
    "    fig.suptitle('Model predictions by subgroup', fontsize=16)\n",
    "    \n",
    "    for i, ax in enumerate(axs.flat):\n",
    "        group = df.loc[i, 'group']\n",
    "        hired = df.loc[i, 'Positive class']\n",
    "        not_hired = df.loc[i, 'Negative class']\n",
    "        ax.pie([hired, not_hired],\n",
    "               labels=['Positive class', 'Negative class'],\n",
    "               autopct='%1.1f%%',\n",
    "               colors=sns.color_palette(\"pastel\")[2:],\n",
    "               startangle=90)\n",
    "        ax.set_title(group)\n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e05306c-f9d4-476b-ad10-553d40d0ae32",
   "metadata": {},
   "source": [
    "The following graphs illustrate the global distribution of the classifier's outcomes, providing a general idea of how the black-box model assigns positive and negative decisions. This representation manifests the noteworthy bias of our black-box model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7600d6f7-093c-4808-b340-8708fca182a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pie_charts_outputs_global(df_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322d6f5d-372b-4b94-9536-906afc5a058a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pie_charts_outputs_subgroups(df_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8920305-7513-4d5b-963d-214f0a946f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the dataframe into the correct format for the SPRT\n",
    "individual_df = get_individual_df(df_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46e2ea7-6dfc-43f2-9332-01bd7ec655be",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(individual_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa91324-912d-4693-849f-6ab451cfb4c6",
   "metadata": {},
   "source": [
    "It is also interesting to see the average output of the system for each subgroup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2e2818-ff9d-416f-89be-e289b7f08972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean output per subgroup\n",
    "individual_df.groupby(['group', 'suitability'])['prediction'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a144ff-900e-411d-beb7-032d1731eb85",
   "metadata": {},
   "source": [
    "### Fairness metrics\n",
    "In the following block, we define the fairness criteria that we will consider in the test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f024da6-1be5-4a57-9ae7-f30532f1d0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sprt_statistical_parity(group1, group2, alpha=0.05, beta=0.2, delta=0.1):\n",
    "    \"\"\"\n",
    "    Statistical parity for SPRT\n",
    "    \"\"\"\n",
    "    # Number of individuals of each group\n",
    "    n1 = len(group1)\n",
    "    n2 = len(group2)\n",
    "\n",
    "    s1 = group1['prediction'].sum()  # Number of predicted positives in group 1\n",
    "    s2 = group2['prediction'].sum()  # Number of predicted positives in group 2\n",
    "\n",
    "    # Observed proportions\n",
    "    p1_hat = s1 / n1\n",
    "    p2_hat = s2 / n2\n",
    "\n",
    "    # Define the hypotheses. Consider p1 > p2 to save runtime. Otherwise, just change the order of the variables.\n",
    "    p0 = (p1_hat + p2_hat) / 2\n",
    "    p1 = p0 + delta/2\n",
    "    p2 = p0 - delta/2\n",
    "\n",
    "    # Likelihood ratio\n",
    "    L0 = (p0**s1 * (1 - p0)**(n1 - s1)) * (p0**s2 * (1 - p0)**(n2 - s2))\n",
    "    L1 = (p1**s1 * (1 - p1)**(n1 - s1)) * (p2**s2 * (1 - p2)**(n2 - s2))\n",
    "    lr = L1 / L0 if L0 > 0 else np.inf\n",
    "\n",
    "    # log likelihood approach\n",
    "    #log_L0 = s1 * np.log(p0) + (n1 - s1) * np.log(1 - p0) + s2 * np.log(p0) + (n2 - s2) * np.log(1 - p0)\n",
    "    #log_L1 = s1 * np.log(p1) + (n1 - s1) * np.log(1 - p1) + s2 * np.log(p2) + (n2 - s2) * np.log(1 - p2)\n",
    "    #log_lr = log_L1 - log_L0\n",
    "    #lr = np.exp(log_lr)\n",
    "    \n",
    "    # Thresholds\n",
    "    A = (1 - beta) / alpha\n",
    "    B = beta / (1 - alpha)\n",
    "    \n",
    "    # Decision\n",
    "    if lr >= A:\n",
    "        return \"Reject H0 (difference detected)\", lr\n",
    "    elif lr <= B:\n",
    "        return \"Accept H0 (no difference)\", lr\n",
    "    else:\n",
    "        return \"Continue sampling\", lr\n",
    "\n",
    "\n",
    "def sprt_equal_opportunity(group1, group2, alpha=0.05, beta=0.2, delta=0.1):\n",
    "    \"\"\"\n",
    "    Equal Opportunity fairness criterion for SPRT.\n",
    "    Compares true positive rates (TPR) between two groups.\n",
    "    \"\"\"\n",
    "\n",
    "    # Filter only the truly positive individuals (Y=1)\n",
    "    g1_pos = group1[group1['suitability'] == 1]\n",
    "    g2_pos = group2[group2['suitability'] == 1]\n",
    "    \n",
    "    n1 = len(g1_pos)\n",
    "    n2 = len(g2_pos)\n",
    "\n",
    "    # Need at least one sample of each group.\n",
    "    if n1 == 0 or n2 == 0:\n",
    "        return \"Continue sampling\" , np.nan\n",
    "\n",
    "    s1 = g1_pos['prediction'].sum()  # Number of predicted positives in group 1\n",
    "    s2 = g2_pos['prediction'].sum()  # Number of predicted positives in group 2\n",
    "\n",
    "    # Observed TPRs\n",
    "    p1_hat = s1 / n1\n",
    "    p2_hat = s2 / n2\n",
    "\n",
    "    # Define the hypotheses. Consider p1 > p2 to save runtime. Otherwise, just change the order of the variables.\n",
    "    p0 = (p1_hat + p2_hat) / 2  # estimador com√∫n bajo H0\n",
    "    p1 = p0 + delta/2\n",
    "    p2 = p0 - delta/2\n",
    "\n",
    "    # Likelihood ratio\n",
    "    L0 = (p0**s1 * (1 - p0)**(n1 - s1)) * (p0**s2 * (1 - p0)**(n2 - s2))\n",
    "    L1 = (p1**s1 * (1 - p1)**(n1 - s1)) * (p2**s2 * (1 - p2)**(n2 - s2))\n",
    "    lr = L1 / L0 if L0 > 0 else np.inf\n",
    "\n",
    "    # Thresholds\n",
    "    A = (1 - beta) / alpha\n",
    "    B = beta / (1 - alpha)\n",
    "\n",
    "    # Decision\n",
    "    if lr >= A:\n",
    "        return \"Reject H0 (difference detected)\", lr\n",
    "    elif lr <= B:\n",
    "        return \"Accept H0 (no difference)\", lr\n",
    "    else:\n",
    "        return \"Continue sampling\", lr\n",
    "\n",
    "\n",
    "def sprt_equalized_odds(group1, group2, alpha=0.05, beta=0.2, delta=0.1):\n",
    "    \"\"\"\n",
    "    Equalized Odds for SPRT\n",
    "      - TPR parity: P(≈∑=1 | Y=1, A=a) vs P(≈∑=1 | Y=1, A=b)\n",
    "      - FPR parity: P(≈∑=1 | Y=0, A=a) vs P(≈∑=1 | Y=0, A=b)\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # Filter only the truly positive individuals (Y=1)\n",
    "    g1_pos = group1[group1['suitability'] == 1]\n",
    "    g2_pos = group2[group2['suitability'] == 1]\n",
    "    \n",
    "    n1 = len(g1_pos)\n",
    "    n2 = len(g2_pos)\n",
    "\n",
    "    # Filter only the truly negative individuals (Y=1)\n",
    "    g1_neg = group1[group1['suitability'] == 0]\n",
    "    g2_neg = group2[group2['suitability'] == 0]\n",
    "    \n",
    "    n1_neg = len(g1_neg)\n",
    "    n2_neg = len(g2_neg)\n",
    "\n",
    "    # Need at least one sample from each group\n",
    "    if n1 == 0 or n2 == 0 or n1_neg == 0 or n2_neg == 0:\n",
    "        return \"Continue sampling\" , np.nan\n",
    "\n",
    "    s1 = g1_pos['prediction'].sum() \n",
    "    s2 = g2_pos['prediction'].sum() \n",
    "\n",
    "    # Observed TPRs\n",
    "    p1_hat = s1 / n1\n",
    "    p2_hat = s2 / n2\n",
    "\n",
    "    # Define the hypotheses. Consider p1 > p2 to save runtime. Otherwise, just change the order of the variables.\n",
    "    p0 = (p1_hat + p2_hat) / 2 \n",
    "    p1 = p0 + delta/2\n",
    "    p2 = p0 - delta/2\n",
    "\n",
    "    # Likelihood ratio\n",
    "    L0 = (p0**s1 * (1 - p0)**(n1 - s1)) * (p0**s2 * (1 - p0)**(n2 - s2))\n",
    "    L1 = (p1**s1 * (1 - p1)**(n1 - s1)) * (p2**s2 * (1 - p2)**(n2 - s2))\n",
    "    lr = L1 / L0 if L0 > 0 else np.inf\n",
    "\n",
    "    # Thresholds\n",
    "    A = (1 - beta) / alpha\n",
    "    B = beta / (1 - alpha)\n",
    "\n",
    "    # Decisi√≥n\n",
    "    if lr >= A:\n",
    "        tpr, lr_tpr = \"Reject H0 (difference detected)\", lr\n",
    "    elif lr <= B:\n",
    "        tpr, lr_tpr = \"Accept H0 (no difference)\", lr\n",
    "    else:\n",
    "        tpr, lr_tpr = \"Continue sampling\", lr\n",
    "\n",
    "\n",
    "\n",
    "    s1_neg = g1_neg['prediction'].sum()  # contratados en grupo 1\n",
    "    s2_neg = g2_neg['prediction'].sum()  # contratados en grupo 2\n",
    "\n",
    "    # Observed TPRs\n",
    "    p1_hat_neg = s1_neg / n1_neg\n",
    "    p2_hat_neg = s2_neg / n2_neg\n",
    "\n",
    "    p0_neg = (p1_hat_neg + p2_hat_neg) / 2  \n",
    "    p1_neg = p0_neg + delta/2\n",
    "    p2_neg = p0_neg - delta/2\n",
    "\n",
    "    # Likelihood ratio\n",
    "    L0_neg = (p0_neg**s1_neg * (1 - p0_neg)**(n1_neg - s1_neg)) * (p0_neg**s2_neg * (1 - p0_neg)**(n2_neg - s2_neg))\n",
    "    L1_neg = (p1_neg**s1_neg * (1 - p1_neg)**(n1_neg - s1_neg)) * (p2_neg**s2_neg * (1 - p2_neg)**(n2_neg - s2_neg))\n",
    "    lr_neg = L1_neg / L0_neg if L0_neg > 0 else np.inf\n",
    "\n",
    "    # Decision\n",
    "    if lr_neg >= A:\n",
    "        fpr, lr_fpr = \"Reject H0 (difference detected)\", lr_neg\n",
    "    elif lr_neg <= B:\n",
    "        fpr, lr_fpr = \"Accept H0 (no difference)\", lr_neg\n",
    "    else:\n",
    "        fpr, lr_fpr = \"Continue sampling\", lr_neg\n",
    "\n",
    "# --- Decisi√≥n global EO\n",
    "    if tpr.startswith(\"Accept\") and fpr.startswith(\"Accept\"):\n",
    "        global_decision = \"Accept H0 (no difference)\"\n",
    "    elif tpr.startswith(\"Reject\") or fpr.startswith(\"Reject\"):\n",
    "        global_decision = \"Reject H0 (difference detected)\"\n",
    "    else:\n",
    "        global_decision = \"Continue sampling\"\n",
    "\n",
    "    return global_decision , np.max([lr_tpr, lr_fpr])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b3627c-cf4e-4f48-8ee7-4daab0335726",
   "metadata": {},
   "source": [
    "### Sequential Probability Ratio Test loop\n",
    "Here we implement the main loop for the sequential test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1a396d-b799-4a1a-9b66-3acccc22101a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_test(df, batch_size=10, max_steps=10, alpha = 0.05, beta = 0.2, delta=0.1, criterion=\"SP\"):\n",
    "    # Create sample pool\n",
    "    pool = individual_df.copy()\n",
    "    pool = pool.sample(frac=1).reset_index(drop=True) # random order\n",
    "\n",
    "    results = []\n",
    "\n",
    "    # Add one individual from each group to avoid divisions by 0 in the first batch.\n",
    "    g1_index = pool[pool['group'] == 'G1'].index[0]\n",
    "    g2_index = pool[pool['group'] == 'G2'].index[0]\n",
    "    \n",
    "    # Evidence set\n",
    "    accumulated = pool.loc[[g1_index, g2_index]]\n",
    "    \n",
    "    pool = pool.drop([g1_index, g2_index])\n",
    "\n",
    "    for step in range(max_steps):\n",
    "        if len(pool) < batch_size:\n",
    "            print(\"No hay suficientes datos para continuar.\")\n",
    "            break\n",
    "\n",
    "        batch = pool.iloc[:batch_size]\n",
    "        pool = pool.iloc[batch_size:]\n",
    "        \n",
    "        accumulated = pd.concat([accumulated, batch], ignore_index=True)\n",
    "\n",
    "        g1 = accumulated[accumulated['group'] == 'G1']\n",
    "        g2 = accumulated[accumulated['group'] == 'G2']\n",
    "        \n",
    "        # Choose test depending on criterion\n",
    "        if criterion == \"SP\":\n",
    "            decision, lr = sprt_statistical_parity(g1, g2, alpha = alpha, beta = beta, delta=delta)\n",
    "        elif criterion == \"EOP\":\n",
    "            decision, lr = sprt_equal_opportunity(g1, g2, alpha = alpha, beta = beta, delta=delta)\n",
    "        elif criterion == \"EO\":\n",
    "            decision, lr = sprt_equalized_odds(g1, g2, alpha = alpha/2, beta = beta, delta=delta) # Bonferroni correction\n",
    "        else:\n",
    "            raise ValueError(\"criterion must be 'SP' or 'EO' or 'EOP'\")\n",
    "\n",
    "        \n",
    "        #print(f\"Step {step+1}: {decision}, LR={lr:.4f}\")\n",
    "        results.append((step+1, decision, lr))\n",
    "        \n",
    "        if decision != \"Continue sampling\":\n",
    "            break\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e846c3b3-3595-4100-9f24-2fc57ee58972",
   "metadata": {},
   "source": [
    "### Neyman-Pearson fixed sample size\n",
    "We use the following formula to calculate the sample size of a proportion tests under the Neyman-Pearson framework:\n",
    "\n",
    "$$n = \\frac{(Z_{1-\\beta} + Z_{1-\\frac{\\alpha}{2}})^2 (\\pi_A(1-\\pi_A)+\\pi_B(1-\\pi_B)) }{(\\pi_B - \\pi_A)^2}$$\n",
    "\n",
    "where\n",
    "- $n$ is the required number of observations in each of the two groups (therefore the total sample size is $2n$).\n",
    "- $Z_{1-x}$ is the value from the standardised normal distribution for which the probability of exceeding it is $x$.‚Äâ\n",
    "- $\\pi_A$ and $\\pi_B$ are the anticipated probabilities of event in groups A and B.‚Äâ\n",
    "- $\\alpha$ is the type I error rate, and $\\beta$ the type II error rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeca2efe-2378-4a9a-9ed0-bade8c58270b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_size_neyman_pearson(suitability, bias, alpha=0.05, beta=0.2):\n",
    "\n",
    "    #Pi\n",
    "    pi_A, pi_B = compute_pi(suitability, bias)\n",
    "    \n",
    "    Z_alpha = norm.ppf(1 - alpha/2)   # para test bilateral\n",
    "    #Z_alpha = norm.ppf(1 - alpha)   # para test one-sided\n",
    "    Z_beta  = norm.ppf(1 - beta)         # 1 - beta\n",
    "\n",
    "    delta = abs(pi_B - pi_A)\n",
    "\n",
    "    var = pi_A * (1 - pi_A) + pi_B * (1 - pi_B)\n",
    "\n",
    "    n = ((Z_alpha + Z_beta)**2 * var) / (delta**2)\n",
    "\n",
    "    # Formula for Sample size per group, therefore the total is x2\n",
    "    total = int(np.ceil(n)) * 2\n",
    "    \n",
    "    return total\n",
    "\n",
    "\n",
    "def sample_size_neyman_pearson_criterion(suitability, bias, alpha=0.05, beta=0.2, criterion = 'SP'):\n",
    "    \n",
    "    #Pi\n",
    "    pi_A, pi_B = compute_pi_criterion(suitability, bias, criterion)\n",
    "    \n",
    "    Z_alpha = norm.ppf(1 - alpha/2)   \n",
    "    Z_beta  = norm.ppf(1 - beta)         # 1 - beta\n",
    "\n",
    "    delta = abs(pi_B - pi_A)\n",
    "\n",
    "    var = pi_A * (1 - pi_A) + pi_B * (1 - pi_B)\n",
    "\n",
    "    n = ((Z_alpha + Z_beta)**2 * var) / (delta**2)\n",
    "\n",
    "    # Formula for Sample size per group, therefore the total is x2\n",
    "    total = int(np.ceil(n)) * 2\n",
    "    \n",
    "    return total\n",
    "\n",
    "def compute_pi(suitability_rates, bias_rates):\n",
    "    pi1 = suitability_rates['group1_prob_suitable'] * bias_rates['group1_suitable'] + (1-suitability_rates['group1_prob_suitable']) * bias_rates['group1_unsuitable']\n",
    "    pi2 = suitability_rates['group2_prob_suitable'] * bias_rates['group2_suitable'] + (1-suitability_rates['group2_prob_suitable']) * bias_rates['group2_unsuitable']\n",
    "    \n",
    "    return pi1, pi2\n",
    "\n",
    "\n",
    "def compute_pi_criterion(suitability_rates, bias_rates, criterion):\n",
    "    # Choose test depending on criterion\n",
    "    if criterion == \"SP\":\n",
    "        pi1 = suitability_rates['group1_prob_suitable'] * bias_rates['group1_suitable'] + (1-suitability_rates['group1_prob_suitable']) * bias_rates['group1_unsuitable']\n",
    "        pi2 = suitability_rates['group2_prob_suitable'] * bias_rates['group2_suitable'] + (1-suitability_rates['group2_prob_suitable']) * bias_rates['group2_unsuitable']\n",
    "    \n",
    "    elif criterion == \"EOP\":\n",
    "        pi1 = bias_rates['group1_suitable']\n",
    "        pi2 = bias_rates['group2_suitable']\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(\"criterion must be 'SP' or 'EOP'\")  \n",
    "        \n",
    "    return pi1, pi2\n",
    "\n",
    "\n",
    "def sample_size_neyman_pearson_equalized_odds(suitability, bias, alpha=0.05, beta=0.2, show_groups=False):\n",
    "\n",
    "    #Pi\n",
    "    pi_A_tpr = bias['group1_suitable']\n",
    "    pi_B_tpr =  bias['group2_suitable']\n",
    "\n",
    "\n",
    "    pi_A_fpr = bias['group1_unsuitable']\n",
    "    pi_B_fpr = bias['group2_unsuitable']\n",
    "\n",
    "    alpha_prime = alpha/2 # Correcci√≥n de Bonferroni\n",
    "    \n",
    "    Z_alpha = norm.ppf(1 - alpha_prime/2)  \n",
    "    Z_beta  = norm.ppf(1 - beta)         # 1 - beta\n",
    "\n",
    "    delta_tpr = abs(pi_B_tpr - pi_A_tpr)\n",
    "    delta_fpr = abs(pi_B_fpr - pi_A_fpr)\n",
    "\n",
    "    var_tpr = pi_A_tpr * (1 - pi_A_tpr) + pi_B_tpr * (1 - pi_B_tpr)\n",
    "    var_fpr = pi_A_fpr * (1 - pi_A_fpr) + pi_B_fpr * (1 - pi_B_fpr)\n",
    "\n",
    "    n_tpr = ((Z_alpha + Z_beta)**2 * var_tpr) / (delta_tpr**2)\n",
    "    n_fpr = ((Z_alpha + Z_beta)**2 * var_fpr) / (delta_fpr**2)\n",
    "\n",
    "    if (show_groups):\n",
    "        print(\"Sample size TPR: \", int(np.ceil(n_tpr))*2, \"\\nSample Size FPR: \" , int(np.ceil(n_fpr))*2)\n",
    "\n",
    "    \n",
    "    n_max = max(n_tpr, n_fpr)\n",
    "    \n",
    "    # Formula for Sample size per group, therefore the total is x2\n",
    "    total = int(np.ceil(n_max)) * 2\n",
    "    \n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2486a64-6290-49cb-bc13-8542d09a8c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_theoretical_sp = sample_size_neyman_pearson_criterion(simulation_suitability_rates, simulation_bias_rates, alpha=0.05, beta=0.2 , criterion= 'SP')\n",
    "\n",
    "print(\"Tama√±o de muestra necesario:\", n_theoretical_sp, \"en total (Statistical Parity).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128cef80-ba7d-429d-beef-4f02ec1a075a",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_theoretical_eo = sample_size_neyman_pearson_criterion(simulation_suitability_rates, simulation_bias_rates, alpha=0.05, beta=0.2 , criterion= 'EOP')\n",
    "\n",
    "print(\"Tama√±o de muestra necesario:\", n_theoretical_eo, \"en total (Equal Opportunity).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1007f044-bcb4-4a2c-bad2-6d8239346aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_theoretical_eodds = sample_size_neyman_pearson_equalized_odds(simulation_suitability_rates, simulation_bias_rates, alpha=0.05, beta=0.2, show_groups=True)\n",
    "\n",
    "print(\"Tama√±o de muestra necesario:\", n_theoretical_eodds, \"en total (Equalized Odds).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f805f8-9ceb-4ef3-924d-c1e5ad37acc8",
   "metadata": {},
   "source": [
    "### Experiments\n",
    "Remember that the sprt loop starts with 2 initial samples. Then, the total number of samples used are *2 + num_steps * batch_size*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac561959-f10e-4793-8098-1ef2064af2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "max_steps = 2000\n",
    "delta = 0.2\n",
    "num_simulations = 10\n",
    "\n",
    "for i in range(num_simulations): \n",
    "    res = run_test(df = individual_df, batch_size = batch_size, max_steps = max_steps, alpha = 0.05, beta = 0.2, delta = delta, criterion = 'SP')\n",
    "    print(\">>> Simulation\", i+1, \":\")\n",
    "    print(res[-1])\n",
    "    print(\"Samples used: \", res[-1][0] * batch_size +2)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b661625-ab9d-4a5d-97b0-99e42eb70945",
   "metadata": {},
   "source": [
    "#### Likelihood ratio Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf5e51e-1077-4835-9c09-12762ded82b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.05\n",
    "beta = 0.2\n",
    "delta = 0.1\n",
    "\n",
    "res = run_test(df = individual_df, batch_size = batch_size, max_steps = max_steps, alpha = alpha, beta = beta, delta = delta, criterion = 'SP')\n",
    "res = pd.DataFrame(res)\n",
    "\n",
    "likelihood_ratios = res.iloc[:,2]\n",
    "\n",
    "\n",
    "x_axis = np.arange(1, len(likelihood_ratios) + 1) * batch_size\n",
    "A = (1 - beta) / alpha\n",
    "B = beta / (1 - alpha)\n",
    "\n",
    "\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10,6))\n",
    "\n",
    "# Y limits\n",
    "center = (A + B) / 2\n",
    "half_range = (A - B) / 2 \n",
    "y_min = center - (2.25*half_range)\n",
    "y_max = center + (2.25*half_range)\n",
    "\n",
    "plt.ylim(y_min, y_max)\n",
    "\n",
    "# Background\n",
    "plt.axhspan(A, y_max, color=\"mistyrose\", alpha=0.5, label=\"Reject H0 region\")\n",
    "plt.axhspan(y_min, B, color=\"lightblue\", alpha=0.5, label=\"Accept H0 region\")\n",
    "plt.axhspan(B, A, color=\"whitesmoke\", alpha=0.5, label=\"Continue sampling\")\n",
    "\n",
    "# LR curve\n",
    "plt.plot(x_axis, likelihood_ratios, marker='o', color=\"black\", label=\"LR Evolution\", linewidth=2)\n",
    "\n",
    "# Threshold lines\n",
    "plt.axhline(A, color=\"red\", linestyle=\"--\", linewidth=1.5)\n",
    "plt.axhline(B, color=\"blue\", linestyle=\"--\", linewidth=1.5)\n",
    "\n",
    "# Labels and title\n",
    "plt.xlabel(\"Total number of evaluated samples\", fontsize=12)\n",
    "plt.ylabel(\"Cumulative Likelihood Ratio\", fontsize=12)\n",
    "plt.title(\"SPRT Decision Regions\", fontsize=14, weight=\"bold\")\n",
    "\n",
    "plt.legend(loc=\"best\")\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddab1d2a-9843-4729-82a9-d7eff5093f10",
   "metadata": {},
   "source": [
    "#### Batch size Values\n",
    "In this section, we try different batch sizes values to check its effect on the SPRT decision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c397d6d-8453-4224-b41f-f01cc7fa71e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "max_steps = 2000\n",
    "delta = 0.2\n",
    "num_simulations = 100\n",
    "\n",
    "alpha = 0.05\n",
    "beta = 0.2\n",
    "\n",
    "\n",
    "resultados_simulaciones = pd.DataFrame()\n",
    "resultados_globales = pd.DataFrame(columns = ['Batch size', 'Mean samples', 'Diff detected'])\n",
    "\n",
    "for j in range(100):\n",
    "    resultados_simulaciones = pd.DataFrame()\n",
    "    for i in range(num_simulations): \n",
    "        res = run_test(df = individual_df, batch_size = batch_size, max_steps = max_steps, alpha = alpha, beta = beta, delta = delta, criterion = 'SP')[-1][0:2]\n",
    "        res = pd.DataFrame(res).transpose()\n",
    "    \n",
    "        col = res.columns[1]\n",
    "    \n",
    "        res.loc[res[col] == 'Reject H0 (difference detected)', col] = 1\n",
    "        res.loc[res[col] == 'Accept H0 (no difference)', col] = 0\n",
    "    \n",
    "        res.iloc[:,0] = res.iloc[:,0] * batch_size + 2\n",
    "        \n",
    "        resultados_simulaciones = pd.concat([resultados_simulaciones, res], ignore_index=True)\n",
    "\n",
    "    resultados_simulaciones.columns = [\"Num_samples\", \"Difference Detected\"]\n",
    "\n",
    "    resultados_globales.loc[len(resultados_globales)] = [batch_size, resultados_simulaciones['Num_samples'].mean(), resultados_simulaciones['Difference Detected'].sum()]\n",
    "    batch_size = batch_size + 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb6d66b-9730-4dc1-970d-1398720bff8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(resultados_globales[0:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ad7e6b-1a62-49ee-ab71-b064652184fb",
   "metadata": {},
   "source": [
    "Regarding the choice of batch size, we remark that it can influence the early trajectory of the first estimates, but its overall impact on the final required sample size and test decision is limited. Using a larger batch size will help mitigate variability and the effect of initial random fluctuations at the start of the experiment (we will see that the average sample size of failed tests is considerably small than the sample size of successful ones). However, in our study we will intentionally maintain a small batch size (each batch consists of two observations) to precisely identify the minimum number of observations required by the SPRT, ensuring the estimates are as accurate as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ee903e-a876-4202-a41c-44175e19c0c2",
   "metadata": {},
   "source": [
    "#### Delta Values\n",
    "A careful choice of the threshold parameter $\\delta$ is essential, as it balances test sensitivity and sample efficiency. On one hand, smaller values of $\\delta$ increase the test's ability to detect subtle biases, but will typically require larger sample sizes. On the other hand, larger $\\delta$ values reduce the number of samples needed, but may miss minor biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b64bc0d-3127-467a-b419-d3813423081b",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "max_steps = 2000\n",
    "num_simulations = 1000\n",
    "\n",
    "alpha = 0.05\n",
    "beta = 0.2\n",
    "\n",
    "\n",
    "resultados_simulaciones = pd.DataFrame()\n",
    "resultados_globales = pd.DataFrame(columns = ['Delta', 'Mean samples', 'Diff detected'])\n",
    "\n",
    "# Small delta\n",
    "delta = 0.05\n",
    "for i in range(num_simulations): \n",
    "    res = run_test(df = individual_df, batch_size = batch_size, max_steps = max_steps, alpha = alpha, beta = beta, delta = delta, criterion = 'SP')[-1][0:2]\n",
    "    res = pd.DataFrame(res).transpose()\n",
    "    \n",
    "    col = res.columns[1]\n",
    "    \n",
    "    res.loc[res[col] == 'Reject H0 (difference detected)', col] = 1\n",
    "    res.loc[res[col] == 'Accept H0 (no difference)', col] = 0\n",
    "    \n",
    "    res.iloc[:,0] = res.iloc[:,0] * batch_size + 2\n",
    "        \n",
    "    resultados_simulaciones = pd.concat([resultados_simulaciones, res], ignore_index=True)\n",
    "\n",
    "resultados_simulaciones.columns = [\"Num_samples\", \"Difference Detected\"]\n",
    "\n",
    "resultados_globales.loc[len(resultados_globales)] = [delta, resultados_simulaciones['Num_samples'].mean(), resultados_simulaciones['Difference Detected'].sum()]\n",
    "\n",
    "print(\"\\nDelta: \", delta,\"\\nNum simulaciones:\" , num_simulations, \"\\nMean sample size: \", resultados_simulaciones['Num_samples'].mean(), \"\\nDiffs detected: \", resultados_simulaciones['Difference Detected'].sum())\n",
    "\n",
    "\n",
    "# Increase delta\n",
    "delta = 0.1\n",
    "resultados_simulaciones = pd.DataFrame()\n",
    "\n",
    "for i in range(num_simulations): \n",
    "    res = run_test(df = individual_df, batch_size = batch_size, max_steps = max_steps, alpha = alpha, beta = beta, delta = delta, criterion = 'SP')[-1][0:2]\n",
    "    res = pd.DataFrame(res).transpose()\n",
    "    \n",
    "    col = res.columns[1]\n",
    "    \n",
    "    res.loc[res[col] == 'Reject H0 (difference detected)', col] = 1\n",
    "    res.loc[res[col] == 'Accept H0 (no difference)', col] = 0\n",
    "    \n",
    "    res.iloc[:,0] = res.iloc[:,0] * batch_size + 2\n",
    "        \n",
    "    resultados_simulaciones = pd.concat([resultados_simulaciones, res], ignore_index=True)\n",
    "\n",
    "resultados_simulaciones.columns = [\"Num_samples\", \"Difference Detected\"]\n",
    "\n",
    "resultados_globales.loc[len(resultados_globales)] = [delta, resultados_simulaciones['Num_samples'].mean(), resultados_simulaciones['Difference Detected'].sum()]\n",
    "\n",
    "print(\"\\nDelta: \", delta,\"\\nNum simulaciones:\" , num_simulations, \"\\nMean sample size: \", resultados_simulaciones['Num_samples'].mean(), \"\\nDiffs detected: \", resultados_simulaciones['Difference Detected'].sum())\n",
    "\n",
    "\n",
    "# Increase delta\n",
    "delta = 0.15\n",
    "resultados_simulaciones = pd.DataFrame()\n",
    "\n",
    "for i in range(num_simulations): \n",
    "    res = run_test(df = individual_df, batch_size = batch_size, max_steps = max_steps, alpha = alpha, beta = beta, delta = delta, criterion = 'SP')[-1][0:2]\n",
    "    res = pd.DataFrame(res).transpose()\n",
    "    \n",
    "    col = res.columns[1]\n",
    "    \n",
    "    res.loc[res[col] == 'Reject H0 (difference detected)', col] = 1\n",
    "    res.loc[res[col] == 'Accept H0 (no difference)', col] = 0\n",
    "    \n",
    "    res.iloc[:,0] = res.iloc[:,0] * batch_size + 2\n",
    "        \n",
    "    resultados_simulaciones = pd.concat([resultados_simulaciones, res], ignore_index=True)\n",
    "\n",
    "resultados_simulaciones.columns = [\"Num_samples\", \"Difference Detected\"]\n",
    "\n",
    "resultados_globales.loc[len(resultados_globales)] = [delta, resultados_simulaciones['Num_samples'].mean(), resultados_simulaciones['Difference Detected'].sum()]\n",
    "\n",
    "print(\"\\nDelta: \", delta,\"\\nNum simulaciones:\" , num_simulations, \"\\nMean sample size: \", resultados_simulaciones['Num_samples'].mean(), \"\\nDiffs detected: \", resultados_simulaciones['Difference Detected'].sum())\n",
    "\n",
    "\n",
    "# Moderate delta\n",
    "delta = 0.20\n",
    "resultados_simulaciones = pd.DataFrame()\n",
    "\n",
    "for i in range(num_simulations): \n",
    "    res = run_test(df = individual_df, batch_size = batch_size, max_steps = max_steps, alpha = alpha, beta = beta, delta = delta, criterion = 'SP')[-1][0:2]\n",
    "    res = pd.DataFrame(res).transpose()\n",
    "    \n",
    "    col = res.columns[1]\n",
    "    \n",
    "    res.loc[res[col] == 'Reject H0 (difference detected)', col] = 1\n",
    "    res.loc[res[col] == 'Accept H0 (no difference)', col] = 0\n",
    "    \n",
    "    res.iloc[:,0] = res.iloc[:,0] * batch_size + 2\n",
    "        \n",
    "    resultados_simulaciones = pd.concat([resultados_simulaciones, res], ignore_index=True)\n",
    "\n",
    "resultados_simulaciones.columns = [\"Num_samples\", \"Difference Detected\"]\n",
    "\n",
    "resultados_globales.loc[len(resultados_globales)] = [delta, resultados_simulaciones['Num_samples'].mean(), resultados_simulaciones['Difference Detected'].sum()]\n",
    "\n",
    "print(\"\\nDelta: \", delta,\"\\nNum simulaciones:\" , num_simulations, \"\\nMean sample size: \", resultados_simulaciones['Num_samples'].mean(), \"\\nDiffs detected: \", resultados_simulaciones['Difference Detected'].sum())\n",
    "\n",
    "\n",
    "# Moderate delta\n",
    "delta = 0.25\n",
    "resultados_simulaciones = pd.DataFrame()\n",
    "\n",
    "for i in range(num_simulations): \n",
    "    res = run_test(df = individual_df, batch_size = batch_size, max_steps = max_steps, alpha = alpha, beta = beta, delta = delta, criterion = 'SP')[-1][0:2]\n",
    "    res = pd.DataFrame(res).transpose()\n",
    "    \n",
    "    col = res.columns[1]\n",
    "    \n",
    "    res.loc[res[col] == 'Reject H0 (difference detected)', col] = 1\n",
    "    res.loc[res[col] == 'Accept H0 (no difference)', col] = 0\n",
    "    \n",
    "    res.iloc[:,0] = res.iloc[:,0] * batch_size + 2\n",
    "        \n",
    "    resultados_simulaciones = pd.concat([resultados_simulaciones, res], ignore_index=True)\n",
    "\n",
    "resultados_simulaciones.columns = [\"Num_samples\", \"Difference Detected\"]\n",
    "\n",
    "resultados_globales.loc[len(resultados_globales)] = [delta, resultados_simulaciones['Num_samples'].mean(), resultados_simulaciones['Difference Detected'].sum()]\n",
    "\n",
    "print(\"\\nDelta: \", delta,\"\\nNum simulaciones:\" , num_simulations, \"\\nMean sample size: \", resultados_simulaciones['Num_samples'].mean(), \"\\nDiffs detected: \", resultados_simulaciones['Difference Detected'].sum())\n",
    "\n",
    "\n",
    "# Moderate delta\n",
    "delta = 0.30\n",
    "resultados_simulaciones = pd.DataFrame()\n",
    "\n",
    "for i in range(num_simulations): \n",
    "    res = run_test(df = individual_df, batch_size = batch_size, max_steps = max_steps, alpha = alpha, beta = beta, delta = delta, criterion = 'SP')[-1][0:2]\n",
    "    res = pd.DataFrame(res).transpose()\n",
    "    \n",
    "    col = res.columns[1]\n",
    "    \n",
    "    res.loc[res[col] == 'Reject H0 (difference detected)', col] = 1\n",
    "    res.loc[res[col] == 'Accept H0 (no difference)', col] = 0\n",
    "    \n",
    "    res.iloc[:,0] = res.iloc[:,0] * batch_size + 2\n",
    "        \n",
    "    resultados_simulaciones = pd.concat([resultados_simulaciones, res], ignore_index=True)\n",
    "\n",
    "resultados_simulaciones.columns = [\"Num_samples\", \"Difference Detected\"]\n",
    "\n",
    "resultados_globales.loc[len(resultados_globales)] = [delta, resultados_simulaciones['Num_samples'].mean(), resultados_simulaciones['Difference Detected'].sum()]\n",
    "\n",
    "print(\"\\nDelta: \", delta,\"\\nNum simulaciones:\" , num_simulations, \"\\nMean sample size: \", resultados_simulaciones['Num_samples'].mean(), \"\\nDiffs detected: \", resultados_simulaciones['Difference Detected'].sum())\n",
    "\n",
    "\n",
    "# Large delta\n",
    "delta = 0.35\n",
    "resultados_simulaciones = pd.DataFrame()\n",
    "\n",
    "for i in range(num_simulations): \n",
    "    res = run_test(df = individual_df, batch_size = batch_size, max_steps = max_steps, alpha = alpha, beta = beta, delta = delta, criterion = 'SP')[-1][0:2]\n",
    "    res = pd.DataFrame(res).transpose()\n",
    "    \n",
    "    col = res.columns[1]\n",
    "    \n",
    "    res.loc[res[col] == 'Reject H0 (difference detected)', col] = 1\n",
    "    res.loc[res[col] == 'Accept H0 (no difference)', col] = 0\n",
    "    \n",
    "    res.iloc[:,0] = res.iloc[:,0] * batch_size + 2\n",
    "        \n",
    "    resultados_simulaciones = pd.concat([resultados_simulaciones, res], ignore_index=True)\n",
    "\n",
    "resultados_simulaciones.columns = [\"Num_samples\", \"Difference Detected\"]\n",
    "\n",
    "resultados_globales.loc[len(resultados_globales)] = [delta, resultados_simulaciones['Num_samples'].mean(), resultados_simulaciones['Difference Detected'].sum()]\n",
    "\n",
    "print(\"\\nDelta: \", delta,\"\\nNum simulaciones:\" , num_simulations, \"\\nMean sample size: \", resultados_simulaciones['Num_samples'].mean(), \"\\nDiffs detected: \", resultados_simulaciones['Difference Detected'].sum())\n",
    "\n",
    "\n",
    "# Large delta\n",
    "delta = 0.4\n",
    "resultados_simulaciones = pd.DataFrame()\n",
    "\n",
    "for i in range(num_simulations): \n",
    "    res = run_test(df = individual_df, batch_size = batch_size, max_steps = max_steps, alpha = alpha, beta = beta, delta = delta, criterion = 'SP')[-1][0:2]\n",
    "    res = pd.DataFrame(res).transpose()\n",
    "    \n",
    "    col = res.columns[1]\n",
    "    \n",
    "    res.loc[res[col] == 'Reject H0 (difference detected)', col] = 1\n",
    "    res.loc[res[col] == 'Accept H0 (no difference)', col] = 0\n",
    "    \n",
    "    res.iloc[:,0] = res.iloc[:,0] * batch_size + 2\n",
    "        \n",
    "    resultados_simulaciones = pd.concat([resultados_simulaciones, res], ignore_index=True)\n",
    "\n",
    "resultados_simulaciones.columns = [\"Num_samples\", \"Difference Detected\"]\n",
    "\n",
    "resultados_globales.loc[len(resultados_globales)] = [delta, resultados_simulaciones['Num_samples'].mean(), resultados_simulaciones['Difference Detected'].sum()]\n",
    "\n",
    "print(\"\\nDelta: \", delta,\"\\nNum simulaciones:\" , num_simulations, \"\\nMean sample size: \", resultados_simulaciones['Num_samples'].mean(), \"\\nDiffs detected: \", resultados_simulaciones['Difference Detected'].sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc37c3a8-9713-4ce1-97a3-a4c4fe1179fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(resultados_globales)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f44b7d2-bd01-412c-963a-77f0d72b3bb7",
   "metadata": {},
   "source": [
    "Let's visualize the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1198a160-a258-49dd-b09e-0c96b942cd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14,6))\n",
    "resultados_globales[\"Delta\"] = resultados_globales[\"Delta\"].astype(str) #Convertir a categor√≠a\n",
    "resultados_globales[\"Diff detected\"] = resultados_globales[\"Diff detected\"] *100 / num_simulations #Convertir a %\n",
    "\n",
    "\n",
    "# Mean samples per Delta\n",
    "axes[0].bar(resultados_globales[\"Delta\"], resultados_globales[\"Mean samples\"], color=\"skyblue\", edgecolor=\"black\")\n",
    "axes[0].set_title(\"Average Sample Size per Delta\", fontsize=14, weight=\"bold\")\n",
    "axes[0].set_xlabel(\"Delta\", fontsize=12)\n",
    "axes[0].set_ylabel(\"Mean Sample Size\", fontsize=12)\n",
    "axes[0].grid(axis=\"y\", linestyle=\"--\", alpha=0.6)\n",
    "\n",
    "# Diffs detected per Delta\n",
    "axes[1].bar(resultados_globales[\"Delta\"], resultados_globales[\"Diff detected\"], color=\"lightcoral\", edgecolor=\"black\")\n",
    "axes[1].set_title(\"Differences Detected per Delta\", fontsize=14, weight=\"bold\")\n",
    "axes[1].set_xlabel(\"Delta\", fontsize=12)\n",
    "axes[1].set_ylabel(\"Differences Detected (%)\", fontsize=12)\n",
    "axes[1].grid(axis=\"y\", linestyle=\"--\", alpha=0.6)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243b937f-de3b-4cf7-b013-d94198e92f5e",
   "metadata": {},
   "source": [
    "### Simulations\n",
    "In this section, we present a series of experiments designed to evaluate the performance of the SPRT as an auditing tool. Our empirical evaluation focuses on quantifying the number of observations required to detect bias under different fairness metrics. For all experiments here, we used a $\\delta$ value of 0.2. Ideally, we could have chosen $\\delta = 0.3$, since we know the bias rates of our black-box model. However, we selected a more conservative choice to better demonstrate the behaviour of our audit framework under conditions where the bias magnitude is unknown, ensuring both a meaningful bias detection and realistic benchmark. For the results presented in the following subsections, ten thousand simulations were run for each metric.\n",
    "#### Statistical Parity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9a1ed8-e6aa-4b48-91c9-b88ad1751a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "max_steps = 2000\n",
    "delta = 0.2\n",
    "num_simulations = 10000\n",
    "\n",
    "alpha = 0.05\n",
    "beta = 0.2\n",
    "\n",
    "\n",
    "resultados_simulaciones = pd.DataFrame()\n",
    "resultados_globales = pd.DataFrame(columns = ['Batch size', 'Mean samples', 'Diff detected'])\n",
    "\n",
    "\n",
    "for i in range(num_simulations): \n",
    "    res = run_test(df = individual_df, batch_size = batch_size, max_steps = max_steps, alpha = alpha, beta = beta, delta = delta, criterion = 'SP')[-1][0:2]\n",
    "    res = pd.DataFrame(res).transpose()\n",
    "    \n",
    "    col = res.columns[1]\n",
    "    \n",
    "    res.loc[res[col] == 'Reject H0 (difference detected)', col] = 1\n",
    "    res.loc[res[col] == 'Accept H0 (no difference)', col] = 0\n",
    "    \n",
    "    res.iloc[:,0] = res.iloc[:,0] * batch_size + 2\n",
    "        \n",
    "    resultados_simulaciones = pd.concat([resultados_simulaciones, res], ignore_index=True)\n",
    "\n",
    "resultados_simulaciones.columns = [\"Num_samples\", \"Difference Detected\"]\n",
    "\n",
    "resultados_globales.loc[len(resultados_globales)] = [batch_size, resultados_simulaciones['Num_samples'].mean(), resultados_simulaciones['Difference Detected'].sum()]\n",
    "\n",
    "print(\"Num simulaciones:\" , num_simulations, \"\\nMean sample size: \", resultados_simulaciones['Num_samples'].mean(), \"\\nDiffs detected: \", resultados_simulaciones['Difference Detected'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9211c8-df92-41b4-b02a-a3db293e96f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultados_simulaciones.loc[:,\"Num_samples\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e57caf9-74a1-4470-80d7-e4636951f738",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Num experiments with less samples than n_theoretical:\", len(resultados_simulaciones[resultados_simulaciones[\"Num_samples\"]<=n_theoretical_sp])) \n",
    "print(\"Num of correct experiments with less samples than n_theoretical:\", len(resultados_simulaciones[(resultados_simulaciones[\"Num_samples\"]<=n_theoretical_sp) & (resultados_simulaciones[\"Difference Detected\"] == 1)]))\n",
    "\n",
    "print(f\"Average sample size of correct experiments: {resultados_simulaciones[resultados_simulaciones['Difference Detected'] == 1]['Num_samples'].mean():.2f}\")\n",
    "print(f\"Average sample size of failed experiments: {resultados_simulaciones[resultados_simulaciones['Difference Detected'] == 0]['Num_samples'].mean():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbfaa27-3328-4f88-bd55-10f015699417",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7, 4))\n",
    "plt.hist(resultados_simulaciones.loc[:,\"Num_samples\"], bins=30, color=\"skyblue\", edgecolor=\"black\", alpha=0.7, label=\"Simulations\")\n",
    "plt.axvline(n_theoretical_sp, color=\"red\", linestyle=\"--\", linewidth=2, label=\"Neyman‚ÄìPearson expected\")\n",
    "\n",
    "plt.title(\"Sample size distribution - Statistical Parity\")\n",
    "plt.xlabel(\"Sample size\")\n",
    "plt.ylabel(\"Frecuency\")\n",
    "plt.legend()\n",
    "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.6)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86aa28e6-bd29-4776-8630-c62b2fe173d8",
   "metadata": {},
   "source": [
    "#### Equal Opportunity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae9abaf-3a0b-4aed-9e29-863dbfee56f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultados_simulaciones_eop = pd.DataFrame()\n",
    "resultados_globales_eop = pd.DataFrame(columns = ['Batch size', 'Mean samples', 'Diff detected'])\n",
    "\n",
    "\n",
    "for i in range(num_simulations): \n",
    "    res = run_test(df = individual_df, batch_size = batch_size, max_steps = max_steps, alpha = alpha, beta = beta, delta = delta, criterion = 'EOP')[-1][0:2]\n",
    "    res = pd.DataFrame(res).transpose()\n",
    "    \n",
    "    col = res.columns[1]\n",
    "    \n",
    "    res.loc[res[col] == 'Reject H0 (difference detected)', col] = 1\n",
    "    res.loc[res[col] == 'Accept H0 (no difference)', col] = 0\n",
    "    \n",
    "    res.iloc[:,0] = res.iloc[:,0] * batch_size + 2\n",
    "        \n",
    "    resultados_simulaciones_eop = pd.concat([resultados_simulaciones_eop, res], ignore_index=True)\n",
    "\n",
    "resultados_simulaciones_eop.columns = [\"Num_samples\", \"Difference Detected\"]\n",
    "\n",
    "resultados_globales_eop.loc[len(resultados_globales_eop)] = [batch_size, resultados_simulaciones_eop['Num_samples'].mean(), resultados_simulaciones_eop['Difference Detected'].sum()]\n",
    "\n",
    "print(\"Num simulaciones:\" , num_simulations, \"\\nMean sample size: \", resultados_simulaciones_eop['Num_samples'].mean(), \"\\nDiffs detected: \", resultados_simulaciones_eop['Difference Detected'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73df65bb-acbd-43fe-81ca-868e80974b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Num experiments with less samples than n_theoretical:\", len(resultados_simulaciones_eop[resultados_simulaciones_eop[\"Num_samples\"]<=n_theoretical_eo])) \n",
    "print(\"Num of correct experiments with less samples than n_theoretical:\", len(resultados_simulaciones_eop[(resultados_simulaciones_eop[\"Num_samples\"]<=n_theoretical_eo) & (resultados_simulaciones_eop[\"Difference Detected\"] == 1)]))\n",
    "\n",
    "\n",
    "print(f\"Average sample size of correct experiments: {resultados_simulaciones_eop[resultados_simulaciones_eop['Difference Detected'] == 1]['Num_samples'].mean():.2f}\")\n",
    "print(f\"Average sample size of failed experiments: {resultados_simulaciones_eop[resultados_simulaciones_eop['Difference Detected'] == 0]['Num_samples'].mean():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8d9b89-9e7e-4d3c-a10d-8f685cb9f627",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_theoretical_eop = sample_size_neyman_pearson_criterion(simulation_suitability_rates, simulation_bias_rates, alpha=0.05, beta=0.2 , criterion= 'EOP')\n",
    "n_theoretical_eop_corrected = n_theoretical_eop/simulation_suitability_rates['group1_prob_suitable']\n",
    "\n",
    "plt.figure(figsize=(7, 4))\n",
    "plt.hist(resultados_simulaciones_eop.loc[:,\"Num_samples\"], bins=30, color=\"skyblue\", edgecolor=\"black\", alpha=0.7, label=\"Simulations\")\n",
    "plt.axvline(n_theoretical_eop, color=\"red\", linestyle=\"--\", linewidth=2, label=\"Neyman‚ÄìPearson expected\")\n",
    "plt.axvline(n_theoretical_eop_corrected, color=\"green\", linestyle=\"--\", linewidth=2, label=\"Neyman‚ÄìPearson corrected\")\n",
    "\n",
    "\n",
    "plt.title(\"Sample size distribution - Equal Opportunity\")\n",
    "plt.xlabel(\"Sample size\")\n",
    "plt.ylabel(\"Frecuency\")\n",
    "plt.legend()\n",
    "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.6)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cacfa9e0-bc04-4719-b0d8-a71ee39b99c9",
   "metadata": {},
   "source": [
    "#### Equalized odds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb40b1b9-d2c4-48b4-b3ff-a6503955a1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultados_simulaciones_eodds = pd.DataFrame()\n",
    "resultados_globales_eodds = pd.DataFrame(columns = ['Batch size', 'Mean samples', 'Diff detected'])\n",
    "\n",
    "for i in range(num_simulations): \n",
    "    res = run_test(df = individual_df, batch_size = batch_size, max_steps = max_steps, alpha = alpha, beta = beta, delta = delta, criterion = 'EO')[-1][0:2]\n",
    "    res = pd.DataFrame(res).transpose()\n",
    "    \n",
    "    col = res.columns[1]\n",
    "    \n",
    "    res.loc[res[col] == 'Reject H0 (difference detected)', col] = 1\n",
    "    res.loc[res[col] == 'Accept H0 (no difference)', col] = 0\n",
    "    res.loc[res[col] == 'Continue sampling', col] = 0\n",
    "    \n",
    "    res.iloc[:,0] = res.iloc[:,0] * batch_size + 2\n",
    "        \n",
    "    resultados_simulaciones_eodds = pd.concat([resultados_simulaciones_eodds, res], ignore_index=True)\n",
    "\n",
    "resultados_simulaciones_eodds.columns = [\"Num_samples\", \"Difference Detected\"]\n",
    "\n",
    "resultados_simulaciones_eodds\n",
    "resultados_globales_eodds.loc[len(resultados_globales_eodds)] = [batch_size, resultados_simulaciones_eodds['Num_samples'].mean(), resultados_simulaciones_eodds['Difference Detected'].sum()]\n",
    "\n",
    "print(\"Num simulaciones:\" , num_simulations, \"\\nMean sample size: \", resultados_simulaciones_eodds['Num_samples'].mean(), \"\\nDiffs detected: \", resultados_simulaciones_eodds['Difference Detected'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df9b215-4770-4690-8e6b-c9f14514d5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Num experiments with less samples than n_theoretical:\", len(resultados_simulaciones_eodds[resultados_simulaciones_eodds[\"Num_samples\"]<=n_theoretical_eodds])) \n",
    "print(\"Num of correct experiments with less samples than n_theoretical:\", len(resultados_simulaciones_eodds[(resultados_simulaciones_eodds[\"Num_samples\"]<=n_theoretical_eodds) & (resultados_simulaciones_eodds[\"Difference Detected\"] == 1)]))\n",
    "\n",
    "\n",
    "print(f\"Average sample size of correct experiments: {resultados_simulaciones_eodds[resultados_simulaciones_eodds['Difference Detected'] == 1]['Num_samples'].mean():.2f}\")\n",
    "print(f\"Average sample size of failed experiments: {resultados_simulaciones_eodds[resultados_simulaciones_eodds['Difference Detected'] == 0]['Num_samples'].mean():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feaf2324-daa8-4237-8531-b22b06bcbc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_theoretical_eodds = sample_size_neyman_pearson_equalized_odds(simulation_suitability_rates, simulation_bias_rates, alpha=0.05, beta=0.2 )\n",
    "n_theoretical_eodds_corrected = n_theoretical_eodds/simulation_suitability_rates['group1_prob_suitable']\n",
    "\n",
    "plt.figure(figsize=(7, 4))\n",
    "plt.hist(resultados_simulaciones_eodds.loc[:,\"Num_samples\"], bins=30, color=\"skyblue\", edgecolor=\"black\", alpha=0.7, label=\"Simulations\")\n",
    "plt.axvline(n_theoretical_eodds, color=\"red\", linestyle=\"--\", linewidth=2, label=\"Neyman‚ÄìPearson expected\")\n",
    "\n",
    "plt.title(\"Sample size distribution - Equalized Odds\")\n",
    "plt.xlabel(\"Sample size\")\n",
    "plt.ylabel(\"Frecuency\")\n",
    "plt.legend()\n",
    "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.6)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d0af0b-e78c-486e-ba6f-72e3463579fa",
   "metadata": {},
   "source": [
    "#### Fair system example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23dd4691-938b-4a97-b388-915ad3734ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "simulation_suitability_rates = {'group1_prob_suitable' : 0.8, \n",
    "                      'group2_prob_suitable' : 0.8}\n",
    "\n",
    "simulation_bias_rates = {'group1_suitable' : 0.9,\n",
    "                            'group1_unsuitable' : 0.2,\n",
    "                            'group2_suitable' : 0.9,\n",
    "                            'group2_unsuitable' : 0.2}\n",
    "\n",
    "df_outputs = black_box_outputs(1000,1000,simulation_suitability_rates, simulation_bias_rates, seed = 42)\n",
    "\n",
    "individual_df = get_individual_df(df_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2707275-3cfe-42a3-8113-7a972525693c",
   "metadata": {},
   "outputs": [],
   "source": [
    "individual_df.groupby(['group', 'suitability'])['prediction'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304b4984-a713-43d6-a17f-0bc3aacaca3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultados_simulaciones_eodds = pd.DataFrame()\n",
    "resultados_globales_eodds = pd.DataFrame(columns = ['Batch size', 'Mean samples', 'Diff detected'])\n",
    "\n",
    "num_simulations = 1000\n",
    "for i in range(num_simulations): \n",
    "    res = run_test(df = individual_df, batch_size = batch_size, max_steps = max_steps, alpha = alpha, beta = beta, delta = delta, criterion = 'EO')[-1][0:2]\n",
    "    res = pd.DataFrame(res).transpose()\n",
    "    \n",
    "    col = res.columns[1]\n",
    "    \n",
    "    res.loc[res[col] == 'Reject H0 (difference detected)', col] = 1\n",
    "    res.loc[res[col] == 'Accept H0 (no difference)', col] = 0\n",
    "    res.loc[res[col] == 'Continue sampling', col] = 0\n",
    "    \n",
    "    res.iloc[:,0] = res.iloc[:,0] * batch_size + 2\n",
    "        \n",
    "    resultados_simulaciones_eodds = pd.concat([resultados_simulaciones_eodds, res], ignore_index=True)\n",
    "\n",
    "resultados_simulaciones_eodds.columns = [\"Num_samples\", \"Difference Detected\"]\n",
    "\n",
    "print(\"Num simulaciones:\" , num_simulations, \"\\nMean sample size: \", resultados_simulaciones_eodds['Num_samples'].mean(), \"\\nDiffs detected: \", resultados_simulaciones_eodds['Difference Detected'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2fc1c1-349d-4eaf-b8b2-2c3862c58497",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Average sample size of correct experiments: {resultados_simulaciones_eodds[resultados_simulaciones_eodds['Difference Detected'] == 1]['Num_samples'].mean():.2f}\")\n",
    "print(f\"Average sample size of failed experiments: {resultados_simulaciones_eodds[resultados_simulaciones_eodds['Difference Detected'] == 0]['Num_samples'].mean():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbede9b-1105-4694-bf70-491a7e906df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7, 4))\n",
    "plt.hist(resultados_simulaciones_eodds.loc[:,\"Num_samples\"], bins=40, color=\"skyblue\", edgecolor=\"black\", alpha=0.7, label=\"Simulations\")\n",
    "\n",
    "\n",
    "plt.title(\"Sample size distribution  - Equalized Odds (fair)\")\n",
    "plt.xlabel(\"Sample size\")\n",
    "plt.ylabel(\"Frecuency\")\n",
    "plt.legend()\n",
    "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.6)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
